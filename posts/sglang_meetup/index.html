<!doctype html><html lang=en><head><title>百度智能云 SGLang Meetup 记录 · where dream begins!</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="bincat"><meta name=description content="
  百度智能云 SGLang Meetup 记录
  
    
    Link to heading
  

今天去百了度组织的 SGLang Meetup，看看明星项目都在解决什么问题，在场好多大佬&mldr;
全场的目标就是集中在GPU利用率以及推理加速上，勉强全部听下来了，很多名词还需要慢慢消化。记录一下印象比较深的几点：

  SGLang 的最新进展
  
    
    Link to heading
  

首先是 SGLang 的最新进展， SGLang 支持了 Diffusion，从文本大模型的优化推广到对于多模态的优化，尤其是图片生成。



还有另一块提到比较多的方向就是投机推理，用小而快的草稿模型来预测多个 Token，再由大模型批量验证，达到推理提速的目的。SGLang 提供的
SpecForge 就专门应用于这种场景。

  推理集群部署
  
    
    Link to heading
  

在部署方面，目前优化的方向还是着眼于DP分离架构下的GPU利用率优化，例如用batch调度来做整体均衡调度，减少并行气泡；另外我比较感兴趣的是Prefill和Decode之间的KV Cache通信问题，也许之前的知识能派上用场。




  DeepSeek V3 系列
  
    
    Link to heading
  

模型适配上面，DeepSeek V3.2 模型受到格外的青睐，厂商在对其进行针对性的优化

  喜闻乐见的茶点环节
  
    
    Link to heading
  





  周边
  
    
    Link to heading
  

最后不得不说 SGLang 还是太懂周边，我被这个帆布袋子狠狠拿捏了"><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="百度智能云 SGLang Meetup 记录"><meta name=twitter:description content="百度智能云 SGLang Meetup 记录 Link to heading 今天去百了度组织的 SGLang Meetup，看看明星项目都在解决什么问题，在场好多大佬…
全场的目标就是集中在GPU利用率以及推理加速上，勉强全部听下来了，很多名词还需要慢慢消化。记录一下印象比较深的几点：
SGLang 的最新进展 Link to heading 首先是 SGLang 的最新进展， SGLang 支持了 Diffusion，从文本大模型的优化推广到对于多模态的优化，尤其是图片生成。
还有另一块提到比较多的方向就是投机推理，用小而快的草稿模型来预测多个 Token，再由大模型批量验证，达到推理提速的目的。SGLang 提供的 SpecForge 就专门应用于这种场景。
推理集群部署 Link to heading 在部署方面，目前优化的方向还是着眼于DP分离架构下的GPU利用率优化，例如用batch调度来做整体均衡调度，减少并行气泡；另外我比较感兴趣的是Prefill和Decode之间的KV Cache通信问题，也许之前的知识能派上用场。
DeepSeek V3 系列 Link to heading 模型适配上面，DeepSeek V3.2 模型受到格外的青睐，厂商在对其进行针对性的优化
喜闻乐见的茶点环节 Link to heading 周边 Link to heading 最后不得不说 SGLang 还是太懂周边，我被这个帆布袋子狠狠拿捏了"><meta property="og:url" content="https://binc4t.github.io/posts/sglang_meetup/"><meta property="og:site_name" content="where dream begins!"><meta property="og:title" content="百度智能云 SGLang Meetup 记录"><meta property="og:description" content="百度智能云 SGLang Meetup 记录 Link to heading 今天去百了度组织的 SGLang Meetup，看看明星项目都在解决什么问题，在场好多大佬…
全场的目标就是集中在GPU利用率以及推理加速上，勉强全部听下来了，很多名词还需要慢慢消化。记录一下印象比较深的几点：
SGLang 的最新进展 Link to heading 首先是 SGLang 的最新进展， SGLang 支持了 Diffusion，从文本大模型的优化推广到对于多模态的优化，尤其是图片生成。
还有另一块提到比较多的方向就是投机推理，用小而快的草稿模型来预测多个 Token，再由大模型批量验证，达到推理提速的目的。SGLang 提供的 SpecForge 就专门应用于这种场景。
推理集群部署 Link to heading 在部署方面，目前优化的方向还是着眼于DP分离架构下的GPU利用率优化，例如用batch调度来做整体均衡调度，减少并行气泡；另外我比较感兴趣的是Prefill和Decode之间的KV Cache通信问题，也许之前的知识能派上用场。
DeepSeek V3 系列 Link to heading 模型适配上面，DeepSeek V3.2 模型受到格外的青睐，厂商在对其进行针对性的优化
喜闻乐见的茶点环节 Link to heading 周边 Link to heading 最后不得不说 SGLang 还是太懂周边，我被这个帆布袋子狠狠拿捏了"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-14T19:56:22+08:00"><meta property="article:modified_time" content="2025-12-14T19:56:22+08:00"><link rel=canonical href=https://binc4t.github.io/posts/sglang_meetup/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.7ee04855f01a7f7a311ce189708f520a1a1d93c249cfed3b23e8c27666fc546e.css integrity="sha256-fuBIVfAaf3oxHOGJcI9SChodk8JJz+07I+jCdmb8VG4=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-light"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://binc4t.github.io/>where dream begins!
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/reading>Reading</a></li><li class=navigation-item><a class=navigation-link href=/friend>Friend</a></li><li class=navigation-item><a class=navigation-link href=/whisper>Whisper</a></li><li class=navigation-item><a class=navigation-link href=/about>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://binc4t.github.io/posts/sglang_meetup/>百度智能云 SGLang Meetup 记录</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-12-14T19:56:22+08:00>December 14, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
One-minute read</span></div></div></header><div class=post-content><h1 id=百度智能云-sglang-meetup-记录>百度智能云 SGLang Meetup 记录
<a class=heading-link href=#%e7%99%be%e5%ba%a6%e6%99%ba%e8%83%bd%e4%ba%91-sglang-meetup-%e8%ae%b0%e5%bd%95><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>今天去百了度组织的 <a href=https://mp.weixin.qq.com/s/BntG0ggzEdUR0ZL_aMq0Ww class=external-link target=_blank rel=noopener>SGLang Meetup</a>，看看明星项目都在解决什么问题，在场好多大佬&mldr;</p><p>全场的目标就是集中在GPU利用率以及推理加速上，勉强全部听下来了，很多名词还需要慢慢消化。记录一下印象比较深的几点：</p><h2 id=sglang-的最新进展>SGLang 的最新进展
<a class=heading-link href=#sglang-%e7%9a%84%e6%9c%80%e6%96%b0%e8%bf%9b%e5%b1%95><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>首先是 SGLang 的最新进展， SGLang 支持了 <strong>Diffusion</strong>，从文本大模型的优化推广到对于多模态的优化，尤其是图片生成。</p><div style=text-align:center><img src=https://pic-1258720617.cos.ap-beijing.myqcloud.com/202512142025506.png width=80%></div><p>还有另一块提到比较多的方向就是<strong>投机推理</strong>，用小而快的草稿模型来预测多个 Token，再由大模型批量验证，达到推理提速的目的。SGLang 提供的
SpecForge 就专门应用于这种场景。</p><h2 id=推理集群部署>推理集群部署
<a class=heading-link href=#%e6%8e%a8%e7%90%86%e9%9b%86%e7%be%a4%e9%83%a8%e7%bd%b2><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>在部署方面，目前优化的方向还是着眼于<strong>DP分离</strong>架构下的GPU利用率优化，例如用batch调度来做整体均衡调度，减少并行气泡；另外我比较感兴趣的是Prefill和Decode之间的<strong>KV Cache通信</strong>问题，也许之前的知识能派上用场。</p><div style=text-align:center><img src=https://pic-1258720617.cos.ap-beijing.myqcloud.com/IMG_1720%202.jpg width=80%></div><h2 id=deepseek-v3-系列>DeepSeek V3 系列
<a class=heading-link href=#deepseek-v3-%e7%b3%bb%e5%88%97><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>模型适配上面，<strong>DeepSeek V3.2</strong> 模型受到格外的青睐，厂商在对其进行针对性的优化</p><h2 id=喜闻乐见的茶点环节>喜闻乐见的茶点环节
<a class=heading-link href=#%e5%96%9c%e9%97%bb%e4%b9%90%e8%a7%81%e7%9a%84%e8%8c%b6%e7%82%b9%e7%8e%af%e8%8a%82><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div style=text-align:center><img src=https://pic-1258720617.cos.ap-beijing.myqcloud.com/IMG_1726.JPG width=80%></div><h2 id=周边>周边
<a class=heading-link href=#%e5%91%a8%e8%be%b9><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>最后不得不说 SGLang 还是太懂周边，我被这个帆布袋子狠狠拿捏了</p><div style=text-align:center><img src=https://pic-1258720617.cos.ap-beijing.myqcloud.com/IMG_1729%20(1).jpg width=80%></div></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2026
bincat
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>